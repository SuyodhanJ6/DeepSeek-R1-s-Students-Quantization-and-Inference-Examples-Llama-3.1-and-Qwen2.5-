{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: auto-round in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (0.4.4)\n",
      "Requirement already satisfied: accelerate in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from auto-round) (1.2.1)\n",
      "Requirement already satisfied: datasets in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from auto-round) (3.2.0)\n",
      "Requirement already satisfied: py-cpuinfo in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from auto-round) (9.0.0)\n",
      "Requirement already satisfied: sentencepiece in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from auto-round) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from auto-round) (1.26.4)\n",
      "Requirement already satisfied: tqdm in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from auto-round) (4.66.5)\n",
      "Requirement already satisfied: packaging in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from auto-round) (24.1)\n",
      "Requirement already satisfied: pillow in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from auto-round) (10.4.0)\n",
      "Requirement already satisfied: numba in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from auto-round) (0.60.0)\n",
      "Requirement already satisfied: tbb in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from auto-round) (2022.0.0)\n",
      "Requirement already satisfied: torch in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from auto-round) (2.5.1)\n",
      "Requirement already satisfied: transformers>=4.38 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from auto-round) (4.48.0)\n",
      "Requirement already satisfied: threadpoolctl in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from auto-round) (3.5.0)\n",
      "Requirement already satisfied: lm-eval<0.5,>=0.4.2 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from auto-round) (0.4.7)\n",
      "Requirement already satisfied: evaluate in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from lm-eval<0.5,>=0.4.2->auto-round) (0.4.3)\n",
      "Requirement already satisfied: jsonlines in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from lm-eval<0.5,>=0.4.2->auto-round) (4.0.0)\n",
      "Requirement already satisfied: numexpr in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from lm-eval<0.5,>=0.4.2->auto-round) (2.10.2)\n",
      "Requirement already satisfied: peft>=0.2.0 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from lm-eval<0.5,>=0.4.2->auto-round) (0.14.0)\n",
      "Requirement already satisfied: pybind11>=2.6.2 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from lm-eval<0.5,>=0.4.2->auto-round) (2.13.6)\n",
      "Requirement already satisfied: pytablewriter in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from lm-eval<0.5,>=0.4.2->auto-round) (1.2.1)\n",
      "Requirement already satisfied: rouge-score>=0.0.4 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from lm-eval<0.5,>=0.4.2->auto-round) (0.1.2)\n",
      "Requirement already satisfied: sacrebleu>=1.5.0 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from lm-eval<0.5,>=0.4.2->auto-round) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn>=0.24.1 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from lm-eval<0.5,>=0.4.2->auto-round) (1.6.1)\n",
      "Requirement already satisfied: sqlitedict in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from lm-eval<0.5,>=0.4.2->auto-round) (2.1.0)\n",
      "Requirement already satisfied: tqdm-multiprocess in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from lm-eval<0.5,>=0.4.2->auto-round) (0.0.11)\n",
      "Requirement already satisfied: zstandard in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from lm-eval<0.5,>=0.4.2->auto-round) (0.23.0)\n",
      "Requirement already satisfied: dill in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from lm-eval<0.5,>=0.4.2->auto-round) (0.3.8)\n",
      "Requirement already satisfied: word2number in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from lm-eval<0.5,>=0.4.2->auto-round) (1.1)\n",
      "Requirement already satisfied: more_itertools in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from lm-eval<0.5,>=0.4.2->auto-round) (10.5.0)\n",
      "Requirement already satisfied: psutil in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from accelerate->auto-round) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from accelerate->auto-round) (6.0.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from accelerate->auto-round) (0.27.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from accelerate->auto-round) (0.4.4)\n",
      "Requirement already satisfied: filelock in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from datasets->auto-round) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from datasets->auto-round) (18.1.0)\n",
      "Requirement already satisfied: pandas in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from datasets->auto-round) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from datasets->auto-round) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from datasets->auto-round) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from datasets->auto-round) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets->auto-round) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from datasets->auto-round) (3.11.11)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from torch->auto-round) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from torch->auto-round) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from torch->auto-round) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from torch->auto-round) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from torch->auto-round) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from torch->auto-round) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from torch->auto-round) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from torch->auto-round) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from torch->auto-round) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from torch->auto-round) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from torch->auto-round) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from torch->auto-round) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from torch->auto-round) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from torch->auto-round) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from torch->auto-round) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from torch->auto-round) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from torch->auto-round) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from sympy==1.13.1->torch->auto-round) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from transformers>=4.38->auto-round) (2024.7.24)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from transformers>=4.38->auto-round) (0.21.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from numba->auto-round) (0.43.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from tbb->auto-round) (1.2.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from aiohttp->datasets->auto-round) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from aiohttp->datasets->auto-round) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from aiohttp->datasets->auto-round) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from aiohttp->datasets->auto-round) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from aiohttp->datasets->auto-round) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from aiohttp->datasets->auto-round) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from aiohttp->datasets->auto-round) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from aiohttp->datasets->auto-round) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from requests>=2.32.2->datasets->auto-round) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from requests>=2.32.2->datasets->auto-round) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from requests>=2.32.2->datasets->auto-round) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from requests>=2.32.2->datasets->auto-round) (2024.7.4)\n",
      "Requirement already satisfied: absl-py in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from rouge-score>=0.0.4->lm-eval<0.5,>=0.4.2->auto-round) (2.1.0)\n",
      "Requirement already satisfied: nltk in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from rouge-score>=0.0.4->lm-eval<0.5,>=0.4.2->auto-round) (3.9.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from rouge-score>=0.0.4->lm-eval<0.5,>=0.4.2->auto-round) (1.16.0)\n",
      "Requirement already satisfied: portalocker in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from sacrebleu>=1.5.0->lm-eval<0.5,>=0.4.2->auto-round) (3.1.1)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from sacrebleu>=1.5.0->lm-eval<0.5,>=0.4.2->auto-round) (0.9.0)\n",
      "Requirement already satisfied: colorama in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from sacrebleu>=1.5.0->lm-eval<0.5,>=0.4.2->auto-round) (0.4.6)\n",
      "Requirement already satisfied: lxml in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from sacrebleu>=1.5.0->lm-eval<0.5,>=0.4.2->auto-round) (5.3.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from scikit-learn>=0.24.1->lm-eval<0.5,>=0.4.2->auto-round) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from scikit-learn>=0.24.1->lm-eval<0.5,>=0.4.2->auto-round) (1.4.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from jinja2->torch->auto-round) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from pandas->datasets->auto-round) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from pandas->datasets->auto-round) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from pandas->datasets->auto-round) (2024.1)\n",
      "Requirement already satisfied: setuptools>=38.3.0 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from pytablewriter->lm-eval<0.5,>=0.4.2->auto-round) (75.6.0)\n",
      "Requirement already satisfied: DataProperty<2,>=1.1.0 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from pytablewriter->lm-eval<0.5,>=0.4.2->auto-round) (1.1.0)\n",
      "Requirement already satisfied: mbstrdecoder<2,>=1.0.0 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from pytablewriter->lm-eval<0.5,>=0.4.2->auto-round) (1.1.3)\n",
      "Requirement already satisfied: pathvalidate<4,>=2.3.0 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from pytablewriter->lm-eval<0.5,>=0.4.2->auto-round) (3.2.3)\n",
      "Requirement already satisfied: tabledata<2,>=1.3.1 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from pytablewriter->lm-eval<0.5,>=0.4.2->auto-round) (1.3.4)\n",
      "Requirement already satisfied: tcolorpy<1,>=0.0.5 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from pytablewriter->lm-eval<0.5,>=0.4.2->auto-round) (0.1.7)\n",
      "Requirement already satisfied: typepy<2,>=1.3.2 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm-eval<0.5,>=0.4.2->auto-round) (1.3.4)\n",
      "Requirement already satisfied: chardet<6,>=3.0.4 in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm-eval<0.5,>=0.4.2->auto-round) (5.2.0)\n",
      "Requirement already satisfied: click in /home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages (from nltk->rouge-score>=0.0.4->lm-eval<0.5,>=0.4.2->auto-round) (8.1.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install auto-round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da84ac3c5511482894bc8cdb86c790c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5763e531a15d4f3193e18ce301dccb87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;20m2025-01-23 15:14:14 INFO utils.py L577: Using GPU device\u001b[0m\n",
      "\u001b[38;20m2025-01-23 15:14:14 INFO autoround.py L230: using torch.float16 for quantization tuning\u001b[0m\n",
      "\u001b[38;20m2025-01-23 15:14:14 INFO autoround.py L300: start to cache block inputs\u001b[0m\n",
      "2025-01-23 15:14:20,577 INFO config.py L54: PyTorch version 2.5.1 available.\n",
      "/home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages/transformers/models/qwen2/modeling_qwen2.py:331: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at ../aten/src/ATen/Context.cpp:208.)\n",
      "  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n",
      "\u001b[38;20m2025-01-23 15:14:29 INFO autoround.py L305: caching done\u001b[0m\n",
      "Quantizing model.layers.0:   0%|          | 0/28 [00:01<?, ?it/s]/home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at ../aten/src/ATen/Context.cpp:208.)\n",
      "  return F.linear(input, self.weight, self.bias)\n",
      "/home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages/auto_round/quantizer.py:269: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at ../aten/src/ATen/Context.cpp:208.)\n",
      "  return F.linear(x, weight, bias)\n",
      "/home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages/torch/autograd/graph.py:825: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at ../aten/src/ATen/Context.cpp:208.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "/home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages/torch/autograd/graph.py:825: UserWarning: Flash Attention defaults to a non-deterministic algorithm. To explicitly enable determinism call torch.use_deterministic_algorithms(True, warn_only=False). (Triggered internally at ../aten/src/ATen/native/transformers/cuda/attention_backward.cu:102.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 31\u001b[0m\n\u001b[1;32m     18\u001b[0m autoround \u001b[38;5;241m=\u001b[39m AutoRound(\n\u001b[1;32m     19\u001b[0m     model, \n\u001b[1;32m     20\u001b[0m     tokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     sym\u001b[38;5;241m=\u001b[39msym\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Perform quantization\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[43mautoround\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Set output directory\u001b[39;00m\n\u001b[1;32m     34\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeepSeek-R1-Distill-Qwen-7B-AutoRound-GPTQ-4bit\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/sft_env/lib/python3.9/site-packages/auto_round/autoround.py:325\u001b[0m, in \u001b[0;36mAutoRound.quantize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m=\u001b[39m total_samples\n\u001b[1;32m    323\u001b[0m             logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforce the train batch size to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 325\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_blocks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnblocks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnblocks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpbar\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_layers(layer_names, all_inputs)\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdump_qinfo_to_layer_config()\n",
      "File \u001b[0;32m~/miniconda3/envs/sft_env/lib/python3.9/site-packages/auto_round/autoround.py:1225\u001b[0m, in \u001b[0;36mAutoRound.quant_blocks\u001b[0;34m(self, model, inputs, block_names, nblocks, device, pbar)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_cpu_mem_usage:\n\u001b[1;32m   1223\u001b[0m         m \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m-> 1225\u001b[0m     q_input, input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mquant_block\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_others\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mq_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mq_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1231\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1232\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   1234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m mv_module_from_gpu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_cpu_mem_usage)\n",
      "File \u001b[0;32m~/miniconda3/envs/sft_env/lib/python3.9/site-packages/auto_round/autoround.py:1109\u001b[0m, in \u001b[0;36mAutoRound.quant_block\u001b[0;34m(self, block, input_ids, input_others, q_input, device)\u001b[0m\n\u001b[1;32m   1104\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1105\u001b[0m         loss \u001b[38;5;241m=\u001b[39m mse_loss(  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m   1106\u001b[0m             output_q\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32), current_output\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m   1107\u001b[0m         )\n\u001b[0;32m-> 1109\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m num_elm\n\u001b[1;32m   1110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_loss_and_backward(scaler, loss)\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from huggingface_hub import login, create_repo, upload_folder\n",
    "import os\n",
    "os.environ[\"HUGGINGFACE_TOKEN\"] = \"\"\n",
    "# Login to Hugging Face (replace with environment variable or secure token management)\n",
    "login()  # You should use your token as an environment variable instead of hardcoding it\n",
    "\n",
    "# Use the 7B model\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "from auto_round import AutoRound\n",
    "\n",
    "# Configure AutoRound parameters\n",
    "bits, group_size, sym = 4, 128, True\n",
    "autoround = AutoRound(\n",
    "    model, \n",
    "    tokenizer,\n",
    "    nsamples=128,\n",
    "    iters=256,\n",
    "    gradient_accumulate_steps=2,\n",
    "    batch_size=4,\n",
    "    bits=bits,\n",
    "    group_size=group_size,\n",
    "    sym=sym\n",
    ")\n",
    "\n",
    "# Perform quantization\n",
    "autoround.quantize()\n",
    "\n",
    "# Set output directory\n",
    "output_dir = \"DeepSeek-R1-Distill-Qwen-7B-AutoRound-GPTQ-4bit\"\n",
    "\n",
    "\n",
    "# Save the quantized model\n",
    "autoround.save_quantized(output_dir, format='auto_gptq', inplace=True)\n",
    "\n",
    "# Create the repository if it doesn't exist\n",
    "# create_repo(repo_id, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d04d932ab91c4c9883b62037837df9e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e4d374cb3943a69180bdb14cd02e42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/SuodhanJ6/DeepSeek-R1-Distill-Qwen-7B-AutoRound-GPTQ-4bit/commit/c772dcbf5d7cfaf09caaa6af62d3418a195b66f3', commit_message='Upload folder using huggingface_hub', commit_description='', oid='c772dcbf5d7cfaf09caaa6af62d3418a195b66f3', pr_url=None, repo_url=RepoUrl('https://huggingface.co/SuodhanJ6/DeepSeek-R1-Distill-Qwen-7B-AutoRound-GPTQ-4bit', endpoint='https://huggingface.co', repo_type='model', repo_id='SuodhanJ6/DeepSeek-R1-Distill-Qwen-7B-AutoRound-GPTQ-4bit'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import login, create_repo, upload_folder\n",
    "login()\n",
    "repo_id = \"SuodhanJ6/DeepSeek-R1-Distill-Qwen-7B-AutoRound-GPTQ-4bit\"\n",
    "output_dir = \"/home/vivek/Documents/prashantPer/perS/DeepSeek-R1-s-Students-Quantization-and-Inference-Examples-Llama-3.1-and-Qwen2.5-/notebooks/DeepSeek-R1-Distill-Qwen-7B-AutoRound-GPTQ-4bit\"\n",
    "# Upload the model to Hugging Face Hub\n",
    "upload_folder(\n",
    "    folder_path=output_dir,\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference with Hugging Face Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers\n",
    "!pip install auto-gptq optimum #if you want to run GPTQ model with HF Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "\n",
    "def test_model(model_name):\n",
    "  model = AutoModelForCausalLM.from_pretrained(\n",
    "      model_name,).to(\"cuda\")\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "  prompt = \"What is the maximum number of whole carrots this rabbit could consume in a 24-hour period?\"\n",
    "  messages = [\n",
    "      {\"role\": \"system\", \"content\": \"You are a very good thinker and reason very well.\"},\n",
    "      {\"role\": \"user\", \"content\": prompt}\n",
    "  ]\n",
    "  text = tokenizer.apply_chat_template(\n",
    "      messages,\n",
    "      tokenize=False,\n",
    "      add_generation_prompt=True\n",
    "  )\n",
    "  inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "  response_ids = model.generate(**inputs, max_new_tokens=4096)\n",
    "  response = tokenizer.batch_decode(response_ids, skip_special_tokens=True)[0]\n",
    "  print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests with Qwen2.5 7B model\n",
    "\n",
    "* Original\n",
    "* Trained by R1\n",
    "* Trained by R1 and quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5fd0d15cfa64987b17d7436cca253d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "You are a very good thinker and reason very well.\n",
      "user\n",
      "What is the maximum number of whole carrots this rabbit could consume in a 24-hour period?\n",
      "assistant\n",
      "To determine the maximum number of whole carrots a rabbit can consume in a 24-hour period, we need to consider the physical and biological limits of the rabbit's digestive system.\n",
      "\n",
      "1. **Digestive System Limitations**:\n",
      "   - A rabbit's stomach is relatively small compared to its body size.\n",
      "   - Carrots contain fiber, which can cause digestive issues if not chewed properly or if too much is consumed at once.\n",
      "   - Overeating carrots can lead to digestive problems such as bloat or diarrhea.\n",
      "\n",
      "2. **Carrot Consumption**:\n",
      "   - On average, a rabbit might eat about 10-15% of its body weight in food daily.\n",
      "   - If a rabbit weighs 2 pounds (about 0.9 kg), it would theoretically consume around 0.2 to 0.3 pounds (about 85-140 grams) of food per day.\n",
      "\n",
      "3. **Carrot Weight**:\n",
      "   - A medium-sized carrot typically weighs about 100 grams (or 0.1 kilograms).\n",
      "\n",
      "4. **Calculation**:\n",
      "   - To find out how many carrots a rabbit weighing 2 pounds could potentially eat in a day:\n",
      "     \\[\n",
      "     \\text{Maximum number of carrots} = \\frac{\\text{Daily consumption}}{\\text{Weight of one carrot}}\n",
      "     \\]\n",
      "     \\[\n",
      "     \\text{Maximum number of carrots} = \\frac{0.3 \\text{ kg}}{0.1 \\text{ kg/carrot}} = 3 \\text{ carrots}\n",
      "     \\]\n",
      "\n",
      "5. **Practical Consideration**:\n",
      "   - While mathematically, a rabbit could potentially eat up to 3 carrots, in practice, rabbits are unlikely to consume more than 1 or 2 carrots due to digestive constraints.\n",
      "   - Even 1 or 2 carrots might be too much for a rabbit's digestive system to handle comfortably.\n",
      "\n",
      "Given these considerations, the maximum number of whole carrots a rabbit could consume in a 24-hour period, while being mindful of digestive health, is:\n",
      "\n",
      "\\[\n",
      "\\boxed{1}\n",
      "\\]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "test_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1449d42f13441c99e42b3257d04fdd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "test_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66d337755066498191cd56e0b8579a28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.41k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:411: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, input, qweight, scales, qzeros, g_idx, bits, maxq):\n",
      "/home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:419: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/home/vivek/miniconda3/envs/sft_env/lib/python3.9/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float16)\n",
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc0a93fea3bf4389bcd30c53a0e2b062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/75.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cfa8357b74b417493322e66f438a0e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f90edb9c39f54db8b5788c21f10c45cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.49G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a160457fbb564f02a5db9e33e22253d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e912e87f40b5461ba9fa7222c868a734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37af51660ebf47ed84e7e9bb65834f12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found modules on cpu/disk. Using Exllama/Exllamav2 backend requires all the modules to be on GPU. Setting `disable_exllama=True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd8e547b8abd4bceb99328401e1d8656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/6.75k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8180671524a49f8b37ad604166a00e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e04726d68f8843b0aa78d93c041fe531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/485 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a very good thinker and reason very well.<｜User｜>What is the maximum number of whole carrots this rabbit could consume in a 24-hour period?<｜Assistant｜><think>\n",
      "Alright, so I need to figure out the maximum number of whole carrots a rabbit could eat in 24 hours. Hmm, okay, let's start by thinking about how rabbits consume plants. I know rabbits are omnivores, so they eat both plants and vegetables, including carrots. \n",
      "\n",
      "First, I should consider the size of the carrots. Carrots vary in size, but let's assume they're average. Maybe about 6 inches long and 1 inch in diameter. That seems reasonable. Now, how much does a rabbit need to eat to stay healthy? I think adult rabbits need a certain number of calories per day. Let me check that. I remember that adult rabbits require about 2,400 to 3,000 calories daily, depending on the variety. Let's go with 2,400 calories for this calculation.\n",
      "\n",
      "Next, I need to figure out how many calories a carrot provides. I'm not exactly sure, but I think a medium carrot has around 15-20 calories. Let me say 18 calories per carrot to be safe. So, if a rabbit needs 2,400 calories, dividing that by 18 gives us about 133 carrots. But wait, rabbits can't eat that many carrots in a day because they can't process that many quickly. Also, they have digestive limits. So, maybe 133 is too high.\n",
      "\n",
      "I should also consider the rabbit's eating capacity. How many carrots can a rabbit eat in an hour? I've heard that rabbits can eat up to 30 carrots in an hour if they're not full. Let me go with 30 carrots per hour. Over 24 hours, that would be 720 carrots. But that seems way too high because 720 carrots would be way more than the 133 needed for calories. There must be another limiting factor.\n",
      "\n",
      "Wait, maybe the rabbit can't eat that many in a day because of their digestive system. Carrots are hard to digest, so a rabbit might only be able to eat a certain number before it's full. I think an average rabbit can eat about 10-15 carrots in an hour before stopping, but I'm not sure. Let's say 12 carrots per hour. That would be 288 carrots in 24 hours. But again, this seems too high because the rabbit's caloric needs would be exceeded much sooner.\n",
      "\n",
      "I should also think about the rabbit's metabolism. They require a certain number of calories to be active, so eating 288 carrots would mean they're not just eating; they're also moving around, which uses energy. So, maybe the actual number is lower because the rabbit can't just eat all day without moving. Let's say they can eat 10 carrots per hour, which is 240 carrots in 24 hours. But 240 carrots still seems too high because the rabbit's body can't process that many.\n",
      "\n",
      "Another angle is to consider the rabbit's teeth. They have molars that can only chomp so much before they need to unchomp. So, maybe they can only eat so many carrots before they have to stop to regrow their teeth. I'm not sure of the exact number, but it's probably in the single digits for a day. Let's say 10 carrots. That seems more reasonable, but I'm not certain.\n",
      "\n",
      "Wait, I should also think about the rabbit's digestive system. Carrots are hard, and rabbits can't digest them properly. They might only be able to eat a certain number before they have to stop. Maybe 5 carrots per hour, which would be 120 carrots in 24 hours. But again, that seems high because the rabbit's caloric needs would be met much faster.\n",
      "\n",
      "I'm getting conflicting numbers here. On one hand, the caloric need suggests about 133 carrots, but the digestive and activity limits suggest much less. I think the key is that rabbits can't eat that many in a day because they can't process it all and their metabolism requires them to be active. So, maybe the maximum is around 10-20 carrots in a day. I've heard that some rabbits eat up to 10 carrots in a day, so that might be the limit.\n",
      "\n",
      "Alternatively, maybe it's around 30 carrots if they're very hungry or if they're young. But I think 30 is pushing it. I think the realistic maximum is around 15-20 carrots in a day for an adult rabbit. So, to sum up, considering the caloric needs, digestive limits, and activity, the maximum number of whole carrots a rabbit could consume in 24 hours is probably around 15-20 carrots.\n",
      "</think>\n",
      "\n",
      "The maximum number of whole carrots a rabbit could consume in a 24-hour period, considering caloric needs, digestive limits, and activity, is approximately 15 to 20 carrots.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"SuodhanJ6/DeepSeek-R1-Distill-Qwen-7B-AutoRound-GPTQ-4bit\"\n",
    "test_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sft_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
